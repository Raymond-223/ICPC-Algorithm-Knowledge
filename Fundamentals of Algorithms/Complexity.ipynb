{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSm54TfmacXE2dVx1l+r9+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raymond-223/ICPC-Algorithm-Knowledge/blob/main/Fundamentals%20of%20Algorithms/Complexity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 复杂度\n",
        "## 基本操作数\n",
        "同一个算法在不同的计算机上运行的速度会有一定的差别，并且实际运行速度难以在理论上进行计算，实际去测量又比较麻烦，所以我们通常考虑的不是算法运行的实际用时，而是算法运行所需要进行的基本操作的数量。\n",
        "\n",
        "在普通的计算机上，加减乘除、访问变量（基本数据类型的变量，下同）、给变量赋值等都可以看作基本操作。\n",
        "\n",
        "对基本操作的计数或是估测可以作为评判算法用时的指标。\n",
        "\n",
        "## 时间复杂度\n",
        "###定义\n",
        "衡量一个算法的快慢，一定要考虑数据规模的大小。所谓数据规模，一般指输入的数字个数、输入中给出的图的点数与边数等等。一般来说，数据规模越大，算法的用时就越长。而在算法竞赛中，我们衡量一个算法的效率时，最重要的不是看它在某个数据规模下的用时，而是看它的用时随数据规模而增长的趋势，即「时间复杂度」。\n",
        "\n",
        "### 原因\n",
        "考虑用时随数据规模变化的趋势的主要原因有以下几点：\n",
        "\n",
        "1. 现代计算机每秒可以处理数亿乃至更多次基本运算，因此我们处理的数据规模通常很大。如果算法 $A$ 在规模为 $n$ 的数据上用时为 $100n$ 而算法 $B$ 在规模为 $n$ 的数据上用时为 $n^{2}$，在数据规模小于 $100$ 时算法 $B$ 用时更短，但在一秒钟内算法 $A$ 可以处理数百万规模的数据，而算法 $B$ 只能处理数万规模的数据。在允许算法执行时间更久时，时间复杂度对可处理数据规模的影响就会更加明显，远大于同一数据规模下用时的影响。\n",
        "\n",
        "2. 我们采用基本操作数来表示算法的用时，而不同的基本操作实际用时是不同的，例如加减法的用时远小于除法的用时。计算时间复杂度而忽略不同基本操作之间的区别以及一次基本操作与十次基本操作之间的区别 （由于一次和十次都属于常数，它们对 “增长趋势” 的影响完全一致），可以消除基本操作间用时不同的影响。\n",
        "\n",
        "当然，算法的运行用时并非完全由输入规模决定，而是也与输入的内容相关。所以，时间复杂度又分为几种，例如：\n",
        "\n",
        "1. 最好时间复杂度，即每个输入规模下用时最短的输入对应的时间复杂度。\n",
        "\n",
        "2. 平均（期望）时间复杂度，即每个输入规模下所有可能输入对应用时的平均值的复杂度（随机输入下期望用时的复杂度）。\n",
        "\n",
        "3. 最坏时间复杂度，即每个输入规模下用时最长的输入对应的时间复杂度。在算法竞赛中，由于输入可以在给定的数据范围内任意给定，我们为保证算法能够通过某个数据范围内的任何数据，一般考虑最坏时间复杂度。\n",
        "\n",
        "##渐近符号的定义\n",
        "渐近符号是函数的阶的规范描述。简单来说，渐近符号忽略了一个函数中增长较慢的部分以及各项的系数（在时间复杂度相关分析中，系数一般被称作「常数」），而保留了可以用来表明该函数增长趋势的重要部分。\n",
        "\n",
        "一个简单的记忆方法是，含等于（非严格）用大写，不含等于（严格）用小写，相等是 $\\Theta$，小于是 $O$，大于是 $\\Omega$。大 $O$ 和小 $o$ 原本是希腊字母 Omicron，由于字形相同，也可以理解为拉丁字母的大 $O$ 和小 $o$。\n",
        "\n",
        "在英文中，词根「-micro-」和「-mega-」常用于表示 10 的负六次方（百万分之一）和六次方（百万），也表示「小」和「大」。小和大也是希腊字母 Omicron 和 Omega 常表示的含义。\n",
        "\n",
        "###  大 $\\Theta$ 符号\n",
        "对于函数 $f(n)$ 和 $g(n)$，$f(n) = \\Theta(g(n))$，当且仅当 $\\exists c_{1}, c_{2}, n_{0} > 0$，使得 $\\forall n \\geqslant n_{0}, 0 \\leqslant c_{1} \\cdot g(n) \\leqslant f(n) \\leqslant c_{2} \\cdot g(n)$。\n",
        "\n",
        "### 大 $O$ 符号\n",
        "我们使用 $O$ 符号来描述一个函数的渐近上界。$f(n) = O(g(n))$，当且仅当 $\\exists c, n_{0}$，使得 $\\forall n \\geqslant n_{0}, 0 \\leqslant f(n) \\leqslant c \\cdot g(n)$。\n",
        "\n",
        "研究时间复杂度时通常会使用 $O$ 符号，因为我们关注的通常是程序用时的上界，而不关心其用时的下界。\n",
        "\n",
        "需要注意的是，这里的「上界」和「下界」是对于函数的变化趋势而言的，而不是对算法本身用时而言的。算法用时的上界对应的是「最坏时间复杂度」而非大 $O$ 记号。所以，使用 $\\Theta$ 记号表示最坏时间复杂度是完全可行的，甚至可以说 $\\Theta$ 比 $O$ 更加精确，而使用 $O$ 记号的主要原因，一是我们有时只能证明时间复杂度的上界而无法证明其下界（这种情况一般出现在较为复杂的算法以及复杂度分析），二是 $O$ 在电脑上输入更方便一些。\n",
        "\n",
        "### 大 $\\Omega$ 符号\n",
        "我们使用 $\\Omega$ 符号来描述一个函数的渐近下界。$f(n) = \\Omega (g(n))$，当且仅当 $\\exists c, n_{0}$，使得 $\\forall n \\geqslant n_{0}, 0 \\leqslant c \\cdot g(n) \\leqslant f(n)$。\n",
        "\n",
        "### 小 $o$ 符号\n",
        "$o$ 符号表示严格小于。$f(n) = o(g(n))$，当且仅当对于任意给定的正数 $c, \\exists n_{0}$，使得 $\\forall n \\geqslant n_{0}, 0 \\leqslant f(n) < c \\cdot g(n)$。\n",
        "\n",
        "### 小 $\\omega$ 符号\n",
        "$\\omega$ 符号表示严格大于。$f(n) = \\omega (g(n))$，当且仅当对于任意给定的正数 $c, \\exists n_{0}$，使得 $\\forall n \\geqslant n_{0}, 0 \\leqslant c \\cdot g(n) < f(n)$。\n",
        "\n",
        "### 常见性质\n",
        "- $f(n) = \\Theta (g(n)) \\Leftrightarrow f(n) = O(g(n)) \\land f(n) = \\Omega (g(n))$\n",
        "\n",
        "- $f_{1}(n) + f_{2}(n) = O(\\max (f_{1}(n), f_{2}(n)))$\n",
        "\n",
        "- $f_{1}(n) \\times f_{2}(n) = O(f_{1}(n) \\times f_{2} (n))$\n",
        "\n",
        "- $\\forall a \\neq 1, \\log_{a} n = O(\\log_{2} n)$。由换底公式可以得知，任何对数函数无论底数为何，都具有相同的增长率，因此渐近时间复杂度中对数的底数一般省略不写。\n",
        "\n",
        "## 常量\n",
        "所有和输入规模无关的量都被视作常量，计算复杂度时可当作 $1$ 来处理。\n",
        "\n",
        "需要注意的是，在进行时间复杂度相关的理论性讨论时，「算法能够解决任何规模的问题」是一个基本假设（当然，在实际中，由于时间和存储空间有限，无法解决规模过大的问题）。因此，能在常量时间内解决数据规模有限的问题（例如，对于数据范围内的每个可能输入预先计算出答案），但这并不意味着使一个算法的时间复杂度变为 $O(1)$，因为它无法处理超出预计算范围的 $n$，且本质依赖 “输入有限”，而非 “算法本身不随 $n$ 增长”。\n",
        "\n",
        "## 主定理 （Master Theorem）\n",
        "我们可以使用 Master Theorem 来快速求得关于递归算法的复杂度。Master Theorem 递推关系式如下\n",
        "\n",
        "$T(n) = aT\\left (\\dfrac{n}{b}\\right ) + f(n)\\ \\ \\ \\ \\forall n > b$\n",
        "\n",
        "那么\n",
        "\n",
        "$T(n) = \\left\\{ \\begin{array}{1}\n",
        "\\hspace{0.5em} \\Theta(n^{\\log_b a}) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ f(n) = O(n^{\\log_{b}(a) - \\epsilon}), \\epsilon > 0\\\\ \\hspace{0.5em} \\Theta(f(n)) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ f(n) = \\Omega(n^{\\log_{b}(a) + \\epsilon}), \\epsilon \\geqslant 0\\\\ \\hspace{0.5em} \\Theta(n^{\\log_b a}\\log^{k + 1}n) \\ \\ \\ \\ f(n) = \\Theta(n^{\\log_{b}(a)} \\log^{k}n), k \\geqslant 0 \\end{array} \\right.$\n",
        "\n",
        "需要注意的是，这里的第二种情况还需要满足正则条件, 即 $af(\\dfrac{n}{b}) \\leqslant cf(n)$，对于某一常数 $c < 1$ 以及足够大的 $n$。\n",
        "\n",
        "###证明\n",
        "####证明思路\n",
        "证明思路是是将规模为 $n$ 的问题，分解为 $a$ 个规模为 $\\left (\\dfrac{n}{b} \\right)$ 的问题，然后依次合并，直到合并到最高层。每一次合并子问题，都需要花费 $f(n)$ 的时间。\n",
        "\n",
        "####具体证明过程\n",
        "对于第 $0$ 层（最高层），合并子问题需要花费 $f(n)$ 的时间\n",
        "\n",
        "对于第 $1$ 层（第一次划分出来的子问题），共有 $a$ 个子问题，每个子问题合并需要花费 $f\\left(\\frac{n}{b}\\right)$ 的时间，所以合并总共要花费 $af\\left(\\frac{n}{b}\\right)$ 的时间。\n",
        "\n",
        "层层递推，我们可以写出类推树如下：\n",
        "\n",
        "这棵树的高度为 $\\log_b n$，共有 $n^{\\log_b a}$ 个叶子，从而 $T(n) = \\Theta(n^{\\log_b a}) + g(n)$，其中 $g(n) = \\displaystyle \\sum_{j = 0}^{\\log_{b}{n - 1}} a^{j} f(n / b^{j})$。\n",
        "\n",
        "针对于第一种情况：$f(n) = O(n^{\\log_b a-\\epsilon})$，因此 $g(n) = O(n^{\\log_b a})$。\n",
        "\n",
        "对于第二种情况而言：首先 $g(n) = \\Omega(f(n))$，又因为 $a f(\\dfrac{n}{b}) \\leq c f(n)$，只要 $c$ 的取值是一个足够小的正数，且 $n$ 的取值足够大，因此可以推导出：$g(n) = O(f(n))$。两侧夹逼可以得出，$g(n) = \\Theta(f(n))$。\n",
        "\n",
        "而对于第三种情况：$f(n) = \\Theta(n^{\\log_b a})$，因此 $g(n) = O(n^{\\log_b a} {\\log n})$。$T(n)$ 的结果可在 $g(n)$ 得出后显然得到。\n",
        "\n",
        "##均摊复杂度\n",
        "均摊分析（Amortized Analysis）是一种用于分析算法和动态数据结构性能的技术。它不仅仅关注单次操作的成本，还通过评估一系列操作的平均成本，为整体性能提供更加准确的评估。均摊分析不涉及概率，且只能确保最坏情况性能的每次操作耗费的平均时间，并不能确认系统的平均性能。在最坏情况下，均摊分析通过将高成本操作的开销分摊到低成本操作上，确保整体操作的平均成本保持在合理范围内。\n",
        "\n",
        "均摊分析通常采用三种主要分析方法：聚合分析、记账分析和势能分析。这些方法各有侧重，分别适用于不同的场景，但它们的共同目标是通过均衡操作成本，优化数据结构在最坏情况下的整体性能表现。\n",
        "\n",
        "### 聚合分析\n",
        "聚合分析（Aggregate Analysis）通过计算一系列操作的总成本，并将其平均到每次操作上，从而得出每次操作的均摊时间复杂度。\n",
        "\n",
        "### 记账分析\n",
        "记账法（Accounting Method）通过为每次操作预先分配一个固定的均摊成本来确保所有操作的总成本不超过这些预分配的成本总和。记账法类似于一种「费用前置支付」的机制，其中较低成本的操作会存储部分费用，以支付未来高成本的操作。\n",
        "\n",
        "### 势能分析\n",
        "势能分析（Potential Method）通过定义一个势能函数（通常表示为 $\\Phi$），度量数据结构的 潜在能量，即系统状态中的预留资源，这些资源可以用来支付未来的高成本操作。势能的变化用于平衡操作序列的总成本，从而确保整个算法的均摊成本在合理范围内。\n",
        "\n",
        "#### 原理\n",
        "首先，定义「状态」$S$ 为某一时刻数据结构的状态，该状态可能包含元素数量、容量、指针等信息，其中定义初始状态为 $S_0$，即未进行任何操作时的状态。\n",
        "\n",
        "其次，定义势能函数 $\\Phi(S)$ 用于度量数据结构状态 $S$ 的势能，其满足以下两个性质：\n",
        "\n",
        "初始势能：在数据结构的初始状态 $S_0$ 下，势能 $\\Phi(S_0) = 0$。\n",
        "非负性：在任意状态 $S$ 下，势能 $\\Phi(S) \\geq 0$。\n",
        "对于每个操作，其均摊成本\n",
        "$\\hat{c}$ 定义为：\n",
        "\n",
        "$\\hat{c} = c + \\Phi(S') - \\Phi(S)$\n",
        "\n",
        "其中 $c$ 为操作的实际成本，$S$ 和 $S'$ 分别表示操作前后的数据结构状态。该公式表明，均摊成本等于实际成本加上势能的变化。如果操作增加了势能（即 $\\Phi(S') > \\Phi(S)$），则均摊成本上升；如果操作消耗了势能（即 $\\Phi(S') < \\Phi(S)$），则均摊成本下降。\n",
        "\n",
        "我们可以通过势能函数来分析一系列操作的总成本。设 $S_1, S_2, \\dots, S_m$ 为从初始状态 $S_0$ 开始，经过 $m$ 次操作后产生的状态序列，$c_i$ 为第 $i$ 次操作的实际开销，那么第 $i$ 次操作的均摊成本 $p_i$ 为：\n",
        "\n",
        "$p_i = c_i + \\Phi(S_i) - \\Phi(S_{i-1})$\n",
        "\n",
        "因此，$m$ 次操作的总时间花销为：\n",
        "\n",
        "$\\displaystyle \\sum_{i=1}^m c_i = \\displaystyle \\sum_{i=1}^m p_i + \\Phi(S_0) - \\Phi(S_m)$\n",
        "\n",
        "由于 $\\Phi(S) \\geq \\Phi(S_0)$，总时间花销的上界为：\n",
        "\n",
        "$\\displaystyle \\sum_{i=1}^m p_i \\geq \\displaystyle \\sum_{i=1}^m c_i$\n",
        "\n",
        "因此，若 $p_i = O(T(n))$，则 $O(T(n))$ 是均摊复杂度的一个上界。\n",
        "\n",
        "##空间复杂度\n",
        "类似地，算法所使用的空间随输入规模变化的趋势可以用「空间复杂度」来衡量。\n"
      ],
      "metadata": {
        "id": "65ZR0bEwzw8K"
      }
    }
  ]
}